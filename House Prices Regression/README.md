# House Prices Regression

This project showcases my work on the "House Prices - Advanced Regression Techniques" Kaggle Competition. It was one of my initial ventures into complex regression problems, with the primary objective of enhancing my understanding and skills in multifaceted regression scenarios.

# Objectives and Learning Outcomes
The primary goal was to familarize myself with various regression techniques and to explore different methodologies for model improvement. Throughout the project, I experimented with:

* L1 Regularization
* Synthetic Minority Over-sampling Technique (SMOTE)
* Random Forest
* Hyperparameter Tuning
* Undersampling
* Shapley Additive Explanations (SHAP)

Despite experimenting with these methods, I ultimately selected XGBoost in its unmodified form, as it yielded the most effective results. The focus of this project extended beyond just model performance; I was trying to learn new skills associated with working with Regression and how to deal with oversampleded model or finding features contributing heavily to the model.

# Reflections and Future Improvement

In retrospect, implementing Principal Component Analysis (PCA) could have been beneficial for reducing the feature space and pinpointing the most impactful features. This insight forms a valuable learning point for future projects, emphasizing the importance of feature selection in model efficiency and interpretability.
